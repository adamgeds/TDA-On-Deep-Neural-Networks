# TDA-On-Deep-Neural-Networks

Capstone project from the Winter 2022 Mathematics Capstone course at The University of Michigan-Dearborn in topological data analysis. 

Acknowledgments:
The enclosed files are the culmination of a semesters-worth of research into how data is processed through a well-trained, deep neural network. 
The majority of this project was based on an article published by Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim in the "Journal of Machine 
Learning Research" titled "Topology of Deep neural Networks." We also used UCI's open-source repository of data sets. In particular, we chose the 
data set of genuine and counterfeit banknotes. It should be mentioned immediately that (due to lack of time and computational power) we were only 
able to investigate one network architecture on one data set. Ideally, we would have liked to test multiple different architectures on multiple 
different data sets. 

Process: (This is described in more detail in the paper.)
We first clean and process the data. Next, we train a network with a 2x20x20x1 architecture. Part of the investigation relied on a practical example of
how TDA could be used to investigate how neural network operate. We chose to look at two different activation functions and try to compare their efficacy.
Specifically, we trained one network using the ReLU activation function and one network using the tanh activation function. All other aspects of the two 
networks were the same (Sigmoid activation on the output layer, Adam optimizer, and Binary Cross Entropy loss function). TensorFlow was used for the
construction of the network. We recreated the feed-forward process using the weights and biases from our well-trained network. Then, we send our data back
through the network while extracting the outputs of each layer. Finally, we apply the Rips Complex to the point cloud generated by the data set and use 
Gudhi to plot persistence barcodes and persistence diagrams to track the birth and death of our simplicies. 

Findings:
We observed that the network trained on the tanh function was significantly better at reducing the Betti numbers of the point cloud generated by the data
set as it passes through the network. In fact, an increase in Betti numbers can be seen in the network trained on the ReLU function. Thus, in this example
and with this particular data set, we conclude that the hyperbolic tangent activation function is more efficient than the ReLU activation function. We can 
also use this process to see how the network is learning. As the data flows through the network, the Betti numbers are being reduced. This is topologically
equivalent to saying that the point cloud generated from the data set is getting grouped tighter and tighter together based on its class. This is what we
would expect to see in a neural network as it processes data.
